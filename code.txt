"""
multi_modal_mri_segmentation_gan.py
Single-file end-to-end pipeline for Multi-modal MRI segmentation using a conditional GAN.

Assumptions:
- Input data: BraTS-style folders or files. For each subject you have:
    * <id>_T1.nii.gz
    * <id>_T2.nii.gz
    * <id>_FLAIR.nii.gz
    * <id>_seg.nii.gz  (ground truth segmentation with labels: 0 background, 1/2/4 tumor subregions)
- The script uses 2D slice-wise training (channels = modalities stacked).
- Uses PyTorch. GPU recommended.

Notes:
- Adjust DATA_ROOT to point to your extracted BraTS dataset folder.
- This code is educational / baseline. For publication-level results you'd add: cross-validation, advanced augmentations, 3D models, better hyperparameter search.
"""

# ----------------------------
# Install dependencies (Colab)
# ----------------------------
# If running locally, install these packages in your environment.
# In Colab, run the following cell first:
#
# !pip install torch torchvision nibabel scikit-image tqdm simpleitk
#
# ----------------------------

import os
import glob
import random
import numpy as np
import nibabel as nib
from skimage.transform import resize
from skimage import exposure
import matplotlib.pyplot as plt
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torchvision.transforms as transforms

# ----------------------------
# Settings / Hyperparameters
# ----------------------------
DATA_ROOT = "/content/BraTS2021_extracted"   # <<-- change to your extracted dataset root
MODALITIES = ["t1", "t2", "flair"]   # filenames should contain these tokens
SEG_TOKEN = "seg"   # token in segmentation file name
IMG_SIZE = 128      # resize each 2D slice to (IMG_SIZE, IMG_SIZE)
BATCH_SIZE = 8
NUM_WORKERS = 2
LR = 2e-4
NUM_EPOCHS = 50
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
LAMBDA_DICE = 1.0
LAMBDA_ADV = 0.5
SEED = 42
SAVE_DIR = "./model_checkpoints"
os.makedirs(SAVE_DIR, exist_ok=True)
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# ----------------------------
# Utilities
# ----------------------------
def load_nii(path):
    img = nib.load(path).get_fdata()
    return img

def normalize_image(img):
    # clip & z-score normalization per slice
    # Add small epsilon to avoid divide-by-zero
    img = np.nan_to_num(img)
    img = (img - np.mean(img)) / (np.std(img) + 1e-8)
    return img

def one_hot_mask(mask, classes=[0,1]):
    # returns binary mask for tumor vs background (binary segmentation)
    # You can extend for multi-class
    out = np.zeros_like(mask, dtype=np.uint8)
    # consider any label >0 as tumor
    out[mask > 0] = 1
    return out

def dice_coeff(pred, target, eps=1e-6):
    # pred, target are torch tensors of shape (N, H, W) or flattened
    intersection = (pred * target).sum(dim=(-2, -1))
    union = pred.sum(dim=(-2, -1)) + target.sum(dim=(-2, -1))
    dice = (2. * intersection + eps) / (union + eps)
    return dice.mean().item()

# ----------------------------
# Dataset (slice-wise)
# ----------------------------
class BraTSSliceDataset(Dataset):
    def __init__(self, data_root, modalities=MODALITIES, img_size=IMG_SIZE, transforms=None, stride=1, subset='all'):
        """
        - data_root: folder containing patient folders OR files.
        - modalities: list of tokens to find modalities.
        - stride: take every nth slice to reduce dataset size.
        - subset: 'all' or list of specific patient ids if you want subset.
        """
        super().__init__()
        self.data_root = data_root
        self.modalities = modalities
        self.img_size = img_size
        self.transforms = transforms
        self.examples = []  # list of (modal_paths, seg_path)
        # find nifti files
        # Approach: search for seg files, then find matching modalities in same folder
        seg_paths = glob.glob(os.path.join(self.data_root, "**", f"*{SEG_TOKEN}*.nii*"), recursive=True)
        for sp in seg_paths:
            folder = os.path.dirname(sp)
            # find modality files in this folder
            mod_paths = []
            found_all = True
            for m in modalities:
                matched = glob.glob(os.path.join(folder, f"*{m}*.nii*"))
                if len(matched) == 0:
                    found_all = False
                    break
                mod_paths.append(matched[0])
            if not found_all:
                continue
            # optionally filter by subset
            if subset!='all':
                pid = os.path.basename(folder)
                if pid not in subset:
                    continue
            # load shapes quickly to compute slices
            try:
                seg_nii = nib.load(sp)
                seg_shape = seg_nii.get_fdata().shape
            except Exception as e:
                print("Error reading", sp, e)
                continue
            # we assume slices along axis 2 (z)
            nz = seg_shape[2]
            for s in range(0, nz, stride):
                self.examples.append((mod_paths, sp, s))
        print(f"Dataset: found {len(self.examples)} 2D slices across {len(seg_paths)} volumes.")
    def __len__(self): return len(self.examples)
    def __getitem__(self, idx):
        mod_paths, seg_path, slice_idx = self.examples[idx]
        # load each modality slice
        slices = []
        for mp in mod_paths:
            img = load_nii(mp)
            # axis assumed (x,y,z). take slice along z
            sl = img[:, :, slice_idx]
            sl = np.nan_to_num(sl)
            # resize to img_size
            sl = resize(sl, (self.img_size, self.img_size), order=1, preserve_range=True, anti_aliasing=True)
            sl = normalize_image(sl)
            slices.append(sl)
        x = np.stack(slices, axis=0).astype(np.float32)  # channels first
        seg3d = load_nii(seg_path)
        seg_sl = seg3d[:, :, slice_idx]
        seg_sl = resize(seg_sl, (self.img_size, self.img_size), order=0, preserve_range=True, anti_aliasing=False)
        mask = one_hot_mask(seg_sl).astype(np.float32)  # binary
        # optional transforms: here just to tensor
        x_t = torch.from_numpy(x)  # (C,H,W)
        y_t = torch.from_numpy(mask).unsqueeze(0)  # (1,H,W)
        return x_t, y_t

# ----------------------------
# Simple U-Net Generator (for conditioned segmentation)
# ----------------------------
class UNetDown(nn.Module):
    def __init__(self, in_ch, out_ch, batch_norm=True):
        super().__init__()
        layers = [nn.Conv2d(in_ch, out_ch, 4, 2, 1, bias=False)]
        if batch_norm:
            layers.append(nn.BatchNorm2d(out_ch))
        layers.append(nn.LeakyReLU(0.2, inplace=True))
        self.model = nn.Sequential(*layers)
    def forward(self,x): return self.model(x)

class UNetUp(nn.Module):
    def __init__(self, in_ch, out_ch, dropout=False):
        super().__init__()
        layers = [nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False),
                  nn.BatchNorm2d(out_ch),
                  nn.ReLU(inplace=True)]
        if dropout:
            layers.append(nn.Dropout(0.5))
        self.model = nn.Sequential(*layers)
    def forward(self,x): return self.model(x)

class UNetGenerator(nn.Module):
    def __init__(self, in_channels=3, out_channels=1, ngf=64):
        super().__init__()
        # encoder
        self.down1 = UNetDown(in_channels, ngf, batch_norm=False)  # 64
        self.down2 = UNetDown(ngf, ngf*2)
        self.down3 = UNetDown(ngf*2, ngf*4)
        self.down4 = UNetDown(ngf*4, ngf*8)
        # bottleneck
        self.bottleneck = nn.Sequential(
            nn.Conv2d(ngf*8, ngf*8, 4, 2, 1),
            nn.ReLU(inplace=True)
        )
        # decoder
        self.up1 = UNetUp(ngf*8, ngf*8, dropout=True)
        self.up2 = UNetUp(ngf*16, ngf*4, dropout=True)
        self.up3 = UNetUp(ngf*8, ngf*2)
        self.up4 = UNetUp(ngf*4, ngf)
        # final
        self.final = nn.Sequential(
            nn.ConvTranspose2d(ngf*2, out_channels, 4, 2, 1),
            nn.Sigmoid()  # output between 0-1 for binary segmentation
        )
    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        b = self.bottleneck(d4)
        u1 = self.up1(b)
        u1 = torch.cat([u1, d4], dim=1)
        u2 = self.up2(u1)
        u2 = torch.cat([u2, d3], dim=1)
        u3 = self.up3(u2)
        u3 = torch.cat([u3, d2], dim=1)
        u4 = self.up4(u3)
        u4 = torch.cat([u4, d1], dim=1)
        out = self.final(u4)
        return out

# ----------------------------
# PatchGAN Discriminator
# ----------------------------
class Discriminator(nn.Module):
    def __init__(self, in_channels=4, ndf=64): # conditional: input = modalities + segmentation (real or fake)
        super().__init__()
        layers = [
            nn.Conv2d(in_channels, ndf, 4, 2, 1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf*2),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf*4),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(ndf*4, 1, 4, 1, 1)  # output patch map
        ]
        self.model = nn.Sequential(*layers)
    def forward(self, x):
        return self.model(x)

# ----------------------------
# Losses: Dice + BCE (for PatchGAN)
# ----------------------------
bce_loss = nn.BCEWithLogitsLoss()
mse_loss = nn.MSELoss()

def dice_loss(pred, target, eps=1e-6):
    # pred: sigmoid outputs between 0-1 (torch)
    pred_flat = pred.view(pred.size(0), -1)
    target_flat = target.view(target.size(0), -1)
    intersection = (pred_flat * target_flat).sum(1)
    denom = pred_flat.sum(1) + target_flat.sum(1)
    loss = 1 - ((2.*intersection + eps) / (denom + eps))
    return loss.mean()

# ----------------------------
# Build dataset & loaders
# ----------------------------
# Create dataset: for speed choose stride=2 or 3 to subsample slices if data is large
train_ds = BraTSSliceDataset(DATA_ROOT, stride=2, subset='all')
# quick split
n = len(train_ds)
idxs = list(range(n))
random.shuffle(idxs)
split = int(0.8*n)
train_idx = idxs[:split]
val_idx = idxs[split:]
from torch.utils.data import Subset
train_loader = DataLoader(Subset(train_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
val_loader = DataLoader(Subset(train_ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)
print("Train batches:", len(train_loader), "Val batches:", len(val_loader))

# ----------------------------
# Init models
# ----------------------------
G = UNetGenerator(in_channels=len(MODALITIES), out_channels=1).to(DEVICE)
D = Discriminator(in_channels=len(MODALITIES) + 1).to(DEVICE)
optimizer_G = optim.Adam(G.parameters(), lr=LR, betas=(0.5, 0.999))
optimizer_D = optim.Adam(D.parameters(), lr=LR, betas=(0.5, 0.999))

# ----------------------------
# Training loop
# ----------------------------
def train_one_epoch(epoch):
    G.train()
    D.train()
    losses = {"G_total":0, "G_adv":0, "G_dice":0, "D_real":0, "D_fake":0}
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    for xb, yb in pbar:
        xb = xb.to(DEVICE)       # (B, C, H, W)
        yb = yb.to(DEVICE)       # (B, 1, H, W)
        bs = xb.size(0)
        # ---------------------
        # Train Discriminator
        # ---------------------
        optimizer_D.zero_grad()
        # real pair: (modalities + real mask)
        real_input = torch.cat([xb, yb], dim=1)
        out_real = D(real_input)
        real_label = torch.ones_like(out_real, device=DEVICE)
        loss_D_real = bce_loss(out_real, real_label)
        # fake pair: (modalities + fake mask)
        with torch.no_grad():
            fake_mask = G(xb)
        fake_input = torch.cat([xb, fake_mask.detach()], dim=1)
        out_fake = D(fake_input)
        fake_label = torch.zeros_like(out_fake, device=DEVICE)
        loss_D_fake = bce_loss(out_fake, fake_label)
        loss_D = (loss_D_real + loss_D_fake) * 0.5
        loss_D.backward()
        optimizer_D.step()
        # ---------------------
        # Train Generator
        # ---------------------
        optimizer_G.zero_grad()
        gen_mask = G(xb)  # sigmoid output
        # adversarial loss: try to fool discriminator into believing fake is real
        fake_input2 = torch.cat([xb, gen_mask], dim=1)
        out_fake_for_G = D(fake_input2)
        # We want D(fake) -> 1
        loss_G_adv = bce_loss(out_fake_for_G, real_label)
        # Dice loss (and optionally BCE for segmentation)
        loss_G_dice = dice_loss(gen_mask, yb)
        # Total generator loss
        loss_G = LAMBDA_ADV * loss_G_adv + LAMBDA_DICE * loss_G_dice
        loss_G.backward()
        optimizer_G.step()
        # Logging
        losses["G_total"] += loss_G.item()
        losses["G_adv"] += loss_G_adv.item()
        losses["G_dice"] += loss_G_dice.item()
        losses["D_real"] += loss_D_real.item()
        losses["D_fake"] += loss_D_fake.item()
        pbar.set_postfix({k: round(v/(pbar.n+1e-9),4) for k,v in losses.items()})
    # average
    for k in losses: losses[k] /= len(train_loader)
    return losses

# ----------------------------
# Validation / Evaluation
# ----------------------------
def evaluate_model():
    G.eval()
    dices = []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(DEVICE); yb = yb.to(DEVICE)
            pred = G(xb)
            pred_bin = (pred > 0.5).float()
            dice = dice_coeff(pred_bin.squeeze(1), yb.squeeze(1))
            dices.append(dice)
    return float(np.mean(dices)) if len(dices)>0 else 0.0

# ----------------------------
# Visualize some predictions
# ----------------------------
def visualize_sample(num=3):
    G.eval()
    samples = []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb = xb.to(DEVICE); yb = yb.to(DEVICE)
            pred = G(xb)
            pred_bin = (pred > 0.5).float()
            # move to cpu for plotting
            xb_c = xb.cpu().numpy()
            yb_c = yb.cpu().numpy()
            pred_c = pred_bin.cpu().numpy()
            for i in range(min(num, xb_c.shape[0])):
                # show modalities as RGB-like (we'll pick three channels)
                m0 = xb_c[i,0]; m1 = xb_c[i,1]; m2 = xb_c[i,2]
                # simple concatenation for figure
                fig, axes = plt.subplots(1,4, figsize=(12,3))
                axes[0].imshow(m2, cmap='gray'); axes[0].set_title("FLAIR")
                axes[1].imshow(m1, cmap='gray'); axes[1].set_title("T2")
                axes[2].imshow(m0, cmap='gray'); axes[2].set_title("T1")
                axes[3].imshow(m2, cmap='gray'); axes[3].imshow(pred_c[i,0], alpha=0.4, cmap='Reds'); axes[3].set_title("Prediction overlay")
                for ax in axes: ax.axis('off')
                plt.show()
            break

# ----------------------------
# Main training
# ----------------------------
best_dice = 0.0
for epoch in range(1, NUM_EPOCHS+1):
    train_losses = train_one_epoch(epoch)
    val_dice = evaluate_model()
    print(f"Epoch {epoch} validation Dice: {val_dice:.4f}")
    # save checkpoint
    torch.save({
        'epoch': epoch,
        'G_state_dict': G.state_dict(),
        'D_state_dict': D.state_dict(),
        'optim_G': optimizer_G.state_dict(),
        'optim_D': optimizer_D.state_dict(),
        'val_dice': val_dice
    }, os.path.join(SAVE_DIR, f"checkpoint_epoch_{epoch}.pth"))
    if val_dice > best_dice:
        best_dice = val_dice
        torch.save(G.state_dict(), os.path.join(SAVE_DIR, "best_generator.pth"))
        torch.save(D.state_dict(), os.path.join(SAVE_DIR, "best_discriminator.pth"))
    print(f"Saved checkpoint. Best val dice: {best_dice:.4f}")
    # visualize a few predictions
    visualize_sample(num=2)

# ----------------------------
# Final evaluation on validation set
# ----------------------------
final_dice = evaluate_model()
print("Final validation Dice:", final_dice)

# ----------------------------
# Save final model & quick inference function
# ----------------------------
torch.save(G.state_dict(), os.path.join(SAVE_DIR, "generator_final.pth"))
print("Saved final generator to", os.path.join(SAVE_DIR, "generator_final.pth"))

def inference_on_volume(generator, volume_modal_paths, out_folder=None):
    """
    volume_modal_paths: list of file paths for modalities [t1, t2, flair]
    Produces and saves predicted segmentation slices as png overlay (if out_folder provided)
    """
    generator.eval()
    imgs = [load_nii(p) for p in volume_modal_paths]
    nz = imgs[0].shape[2]
    preds = []
    with torch.no_grad():
        for s in range(nz):
            slices = [resize(img[:,:,s], (IMG_SIZE, IMG_SIZE), preserve_range=True) for img in imgs]
            # normalize per-slice
            slices_n = [normalize_image(sl) for sl in slices]
            x = np.stack(slices_n, axis=0).astype(np.float32)
            x_t = torch.from_numpy(x).unsqueeze(0).to(DEVICE)
            pred = generator(x_t)
            pred_bin = (pred>0.5).cpu().numpy()[0,0]
            preds.append(pred_bin)
    # optionally save overlays
    if out_folder:
        os.makedirs(out_folder, exist_ok=True)
        for i,p in enumerate(preds):
            plt.imsave(os.path.join(out_folder, f"pred_{i:03d}.png"), p, cmap='gray')
    return preds


"""